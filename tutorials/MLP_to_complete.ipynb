{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_tags.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mipSoOVlavkb"
      },
      "source": [
        "# IVADO/MILA DEEP LEARNING SCHOOL\n",
        "# 4th edition (Fall 2019)\n",
        "# Tutorial : Categorical data with multilayer perceptron (MLP)\n",
        "\n",
        "## Authors: \n",
        "\n",
        "Arsène Fansi Tchango <arsene.fansi.tchango@mila.quebec>\n",
        "\n",
        "Gaétan Marceau Caron <gaetan.marceau.caron@mila.quebec>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sLHwvggEZERd"
      },
      "source": [
        "## Preface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JKNGtQkkohiM"
      },
      "source": [
        "This tutorial introduces the practical aspects of Deep Learning through the realization of a simple end-to-end project. We will use the deep learning library <a href=\"https://pytorch.org/\"> `PyTorch`</a>, which is well-known for its ease of use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NOD70vdvvtin"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq9FwFVnQihX",
        "colab_type": "text"
      },
      "source": [
        "## Loading packages and using GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reCpBfp1Qcrt",
        "colab_type": "text"
      },
      "source": [
        "Before we start, we install the necessary packages for the tutorial by using pip. To do this, execute the following cell by selecting it and using `shift+Enter`. This step may take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c5AlBPjnvzNh",
        "outputId": "df12e24a-9cb7-4529-b259-4beb4504bfe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "!pip3 install 'torch==1.1.0' 'torchvision==0.3.0' 'Pillow==4.3.0' 'matplotlib==3.0.3'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.1.0 in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: torchvision==0.3.0 in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: Pillow==4.3.0 in /usr/local/lib/python3.6/dist-packages (4.3.0)\n",
            "Requirement already satisfied: matplotlib==3.0.3 in /usr/local/lib/python3.6/dist-packages (3.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.1.0) (1.16.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0) (1.12.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.3.0) (0.46)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (2.4.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.0.3) (41.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "djF9gjzLwsDB"
      },
      "source": [
        "Now, import all the modules we will use for this tutorial by running the next cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w9LnNnxBw0wC",
        "outputId": "70578f22-5596-4f74-b426-d57a90ab5feb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "seed = 1234\n",
        "np.random.seed(seed) # Set the random seed of numpy for the data split.\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
        "\n",
        "\n",
        "print(\"Torch version: \", torch.__version__)\n",
        "print(\"GPU Available: {}\".format(use_gpu))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch version:  1.1.0\n",
            "GPU Available: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZKzgFV9Favkt"
      },
      "source": [
        "## PyTorch in a nutshell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vrus_-F0avkt"
      },
      "source": [
        "*PyTorch* is a Python library that supports a vibrant ecosystem of tools and libraries for ML in vision, NLP, and more. It provides two high-level features:\n",
        "<ul>\n",
        "<li> operations on <a href=\"https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\">tensors</a> (such as NumPy) with GPU support,</li>\n",
        "<li> operations for creating and optimizing computational graphs with an automatic differentiation system called <a href=\"https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py\">Autograd</a>.</li>\n",
        "</ul>\n",
        "\n",
        "<a href=\"https://pytorch.org/docs/stable/torch.html\">PyTorch docs</a> contain the API documentation and <a href=\"https://pytorch.org/tutorials/\">many tutorials</a>.\n",
        "Also, PyTorch offers several data processing utilities. One of these utilities is the class <a href=\"http://pytorch.org/docs/master/data.html#\"> `torch.utils.data.Dataset`</a> which offers an easy to use interface to handle a data set. For more information, please refer to the following urls: \n",
        "<ul>\n",
        "<li>PyTorch data sets: <a href=\"http://pytorch.org/docs/master/data.html\"> PyTorch - datasets</a>.</li>\n",
        "<li>A tutorial for loading data: <a href=\"http://pytorch.org/tutorials/beginner/data_loading_tutorial.html\"> PyTorch - data loading tutorial</a>.</li>\n",
        "</ul>\n",
        "\n",
        "<a href=\"http://pytorch.org/docs/master/cuda.html#module-torch.cuda\">`torch.cuda`</a> is a package that provides the same functions as CPU tensors but for  CUDA tensors, which are used for GPU computing. <a href=\"http://pytorch.org/docs/master/cuda.html#torch.cuda.is_available\">`torch.cuda.is_available()`</a> returns a boolean indicating if CUDA is currently available. Finally, we recommend using a `device` variable that identifies the device on which you want to perform computations. We can assign a tensor to a device with the method `.to(device)`. By default, the tensors are CPU tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qm122vNmq92L"
      },
      "source": [
        "## Ingredients for a proof of concept (POC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nqvhR0ebavmE"
      },
      "source": [
        "To realize a ML POC, you need:\n",
        "<ul>\n",
        "<li>a task description as well as data to support it,</li>\n",
        "<li>an evaluation metric to assess the performance of a model,</li>\n",
        "<li>a model description,</li>\n",
        "<li>a cost function to be optimized,</li>\n",
        "<li>an optimizer that adjusts the parameters of the model.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y8_pfpu2f6AO"
      },
      "source": [
        "# How to prepare the dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B5piZxYUhSzq"
      },
      "source": [
        "Our task is to predict whether or not a passenger survived the sinking of the Titanic based on passenger data only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y4GuYNDFavlU"
      },
      "source": [
        "## Titanic dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NiOJx2ytavlU"
      },
      "source": [
        "We can download the Titanic dataset at the following address: https://github.com/afansi/winterschool18/blob/master/titanic3.csv?raw=true.<br/>\n",
        "This dataset provides information on the fate of 1309 passengers of the first and only journey of the liner \"RMS Titanic\", summarized by economic status (class), gender, age, family information, and survival. The Kaggle platform also uses this dataset as an introduction to classical machine learning. Here, we use it to introduce more advanced concepts related to PyTorch and Deep Learning.\n",
        "\n",
        "We use the library <a href=\"https://pandas.pydata.org/\">Pandas</a> to load the dataset into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bX_RSiffavlW",
        "outputId": "9be4e28f-7eb4-4b6a-c288-1e92959224dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "titanic_df = pd.read_csv(\n",
        "    'https://github.com/afansi/winterschool18/blob/master/titanic3.csv?raw=true', \n",
        "    sep='\\t', \n",
        "    index_col=None, \n",
        "    na_values=['NA']\n",
        ")\n",
        "\n",
        "# a snapshot of the first 5 data points\n",
        "titanic_df.head()\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pclass</th>\n",
              "      <th>survived</th>\n",
              "      <th>name</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>ticket</th>\n",
              "      <th>fare</th>\n",
              "      <th>cabin</th>\n",
              "      <th>embarked</th>\n",
              "      <th>boat</th>\n",
              "      <th>body</th>\n",
              "      <th>home.dest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Allen, Miss. Elisabeth Walton</td>\n",
              "      <td>female</td>\n",
              "      <td>29.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24160</td>\n",
              "      <td>211.3375</td>\n",
              "      <td>B5</td>\n",
              "      <td>S</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>St Louis, MO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Allison, Master. Hudson Trevor</td>\n",
              "      <td>male</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Allison, Miss. Helen Loraine</td>\n",
              "      <td>female</td>\n",
              "      <td>2.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Allison, Mr. Hudson Joshua Creighton</td>\n",
              "      <td>male</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>135.0</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
              "      <td>female</td>\n",
              "      <td>25.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pclass  survived  ...   body                        home.dest\n",
              "0       1         1  ...    NaN                     St Louis, MO\n",
              "1       1         1  ...    NaN  Montreal, PQ / Chesterville, ON\n",
              "2       1         0  ...    NaN  Montreal, PQ / Chesterville, ON\n",
              "3       1         0  ...  135.0  Montreal, PQ / Chesterville, ON\n",
              "4       1         0  ...    NaN  Montreal, PQ / Chesterville, ON\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yj88WmCmavlf"
      },
      "source": [
        "**The meaning of the different columns (features) is as follows**:\n",
        "\n",
        "<ol>\n",
        "\n",
        "  <li> <b>pclass</b>: Passenger class (1 = first; 2 = second; 3 = third) </li>\n",
        "  <li> <b>survived</b>: Survived? (0 = no; 1 = yes) </li>\n",
        "  <li> <b>name</b>: Name </li>\n",
        "  <li> <b>sex</b>: Sex </li>\n",
        "  <li> <b>age</b>: Age </li>\n",
        "  <li> <b>sibsp</b>: Number of brothers, sisters, or spouses onboard </li>\n",
        "  <li> <b>parch</b>: Number of parents or children onboard </li>\n",
        "  <li> <b>ticket</b>: Ticket number </li>\n",
        "  <li> <b>fare</b>: Passenger fare </li>\n",
        "  <li> <b>cabin</b>: Cabin number </li>\n",
        "  <li> <b>embarked</b>: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) </li>\n",
        "  <li> <b>boat</b>: Lifeboat (if the passenger survived) </li>\n",
        "  <li> <b>body</b>: Body number (if the passenger did not survive and his body was found) </li>\n",
        "  <li> <b>home.dest</b>: the passenger's destination </li>\n",
        " </ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u2ed5fozqjce"
      },
      "source": [
        "## Data pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "__vcZhPnavlg"
      },
      "source": [
        "### Feature selection\n",
        "Some features are not relevant to the task, for example:\n",
        "<ol>\n",
        "  <li> <b>name</b>: Name </li>\n",
        "  <li> <b>ticket</b>: Ticket number </li>\n",
        "  <li> <b>cabin</b>: Cabin number </li>\n",
        "  <li> <b>home.dest</b>: Passenger's destination </li>\n",
        " </ol>\n",
        " \n",
        "Other features give away the label to be predicted and including them would be cheating:\n",
        "<ol>\n",
        "  <li> <b>boat</b>: Lifeboat (if the passenger survived) </li>\n",
        "  <li> <b>body</b>: Body number (if the passenger did not survive and his body was found) </li>\n",
        " </ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JJ0--SDpavlg",
        "outputId": "3cb1a25a-75e9-471c-9705-97bb195ca873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "titanic_preprocess_df = pd.read_csv(\n",
        "    'https://github.com/afansi/winterschool18/blob/master/titanic_prepocess.csv?raw=true', \n",
        "    sep=',', \n",
        "    index_col=None\n",
        ")\n",
        "\n",
        "titanic_preprocess_df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>pclass_1</th>\n",
              "      <th>pclass_2</th>\n",
              "      <th>pclass_3</th>\n",
              "      <th>sex_female</th>\n",
              "      <th>sex_male</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>embarked_C</th>\n",
              "      <th>embarked_Q</th>\n",
              "      <th>embarked_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>29.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>211.3375</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>25.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   survived  pclass_1  pclass_2  ...  embarked_C  embarked_Q  embarked_S\n",
              "0         1         1         0  ...           0           0           1\n",
              "1         1         1         0  ...           0           0           1\n",
              "2         0         1         0  ...           0           0           1\n",
              "3         0         1         0  ...           0           0           1\n",
              "4         0         1         0  ...           0           0           1\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MckYm0M_xhR",
        "colab_type": "text"
      },
      "source": [
        " ### Feature encoding\n",
        " \n",
        "Some features are **categorical variables**, which means that they can take a finite number of values.\n",
        " <ol>\n",
        "  <li> <b>pclass</b>: Passenger Class </li>\n",
        "  <li> <b>sex</b>: Sex </li>\n",
        "  <li> <b>embarked</b>: Port of embarkation </li>\n",
        " </ol>\n",
        "\n",
        "To process categorical variables, we need to encode them in a way that does not imply an arbitrary order such as using natural numbers (e.g., 1, 2, 3). <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">One-hot encoding</a> is a way to achieve it. We can download the pre-processed dataset at the following address: https://github.com/afansi/winterschool18/blob/master/titanic_prepocess.csv?raw=true.\n",
        "<br>The meaning of the encoded variables is as follows:\n",
        "\n",
        "<ol>\n",
        "  <li> <b>survived</b>: Survived? (0 = no; 1 = yes) </li>\n",
        "  <li> <b>pclass_1</b>: (1 if passenger in first class; 0 if not) </li>\n",
        "  <li> <b>pclass_2</b>: (1 if passenger in second class; 0 if not) </li>\n",
        "  <li> <b>pclass_3</b>: (1 if passenger in third class; 0 if not) </li>\n",
        "  <li> <b>sex_female</b>: (1 if passenger is female; 0 if not) </li>\n",
        "  <li> <b>sex_male</b>: (1 if passenger is male; 0 if not) </li>\n",
        "  <li> <b>age</b>: Age </li>\n",
        "  <li> <b>sibsp</b>: Number of brothers, sisters, or spouses onboard </li>\n",
        "  <li> <b>parch</b>: Number of parents or children onboard </li>\n",
        "  <li> <b>fare</b>: Passenger fare </li>\n",
        "  <li> <b>embarked_C</b>: (1 if Port of embarkation = Cherbourg (C); 0 otherwise) </li> \n",
        "  <li> <b>embarked_Q</b>: (1 if Port of embarkation = Queenstown (Q); 0 otherwise) </li> \n",
        "  <li> <b>embarked_S</b>: (1 if Port of embarkation = Southampton (S); 0 otherwise)</li> \n",
        " </ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QJcs6PUTavlm"
      },
      "source": [
        "## Train / validation / test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bjbgvffmavlo"
      },
      "source": [
        "At this point, we need to divide the dataset into three subsets:\n",
        "\n",
        "<ol>\n",
        "<li> <b> Train</b> (usually 60% of the dataset): used to train the classification model. </li>   \n",
        "<li> <b> Validation</b> (generally 20% of the dataset): used to evaluate hyper-parameters on held-out data. </li>   \n",
        "<li> <b> Test</b> (usually 20% of the dataset): used to evaluate the generalization performance of the chosen model on held-out data. </li>\n",
        "</ol>\n",
        "\n",
        "We use the <a href=\"https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.split.html\">numpy.split function</a> to separate our dataset into subsets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GBmL8VBOavlo",
        "colab": {}
      },
      "source": [
        "train, validate, test = np.split(\n",
        "    titanic_preprocess_df.sample(frac=1, random_state=seed), \n",
        "    [int(.6*len(titanic_preprocess_df)), int(.8*len(titanic_preprocess_df))])\n",
        "\n",
        "# Remove the label column from X and create a label vectors.\n",
        "X_train = train.drop(['survived'], axis=1).values\n",
        "y_train = train['survived'].values\n",
        "\n",
        "X_val = validate.drop(['survived'], axis=1).values\n",
        "y_val = validate['survived'].values\n",
        "\n",
        "X_test = test.drop(['survived'], axis=1).values\n",
        "y_test = test['survived'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wv74TbIWavlr"
      },
      "source": [
        "## Datasets in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9_LJtG-Xavlt"
      },
      "source": [
        "We will use the subclass <b><a href=\"http://pytorch.org/docs/master/data.html#\"> `torch.utils.data.TensorDataset`</a> </b> to manipulate together the features and targets of a dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1JtT4tV7avlt",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
        "\n",
        "val_dataset = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n",
        "\n",
        "test_dataset = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "obEPHnlTavkc"
      },
      "source": [
        "# How to define the learning algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qhN5GL6Gavks"
      },
      "source": [
        "A multilayer perceptron (MLP) is a simple computational graph composed of \"hidden layers,\" which are defined by two modules: a *linear transformation* followed by a *non-linearity*. The result of a hidden layer is a vector called *a distributed representation* where each component is associated with a hidden unit.\n",
        "\n",
        "To train this model, we need to define:\n",
        "<ul>\n",
        "<li>the network architecture by choosing the non-linear function and the number of hidden units per layer, </li>\n",
        "<li>the cost function and optimizer. </li>\n",
        "</ul>\n",
        "\n",
        "To solve our task, we will use a MLP with the following properties:\n",
        " <ul>\n",
        " <li> the input dimension of the model is 12,</li>\n",
        " <li> the output dimension of the model is 2,</li>\n",
        " <li> the first dimension of the output is the probability of death and the second dimension is the probability of survival,</li>\n",
        "  <li> the number of hidden layers is 3, </li>\n",
        " <li> the dimensions of the hidden layers are 20, 40, 20 respectively, </li>\n",
        " <li> the activation function is a ReLu for all hidden layers. </li>\n",
        " </ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "701t0e-ravkr"
      },
      "source": [
        "## How to define a model in PyTorch?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m4F5cyijavkv"
      },
      "source": [
        "The <a href=\"https://pytorch.org/docs/stable/nn.html\">PyTorch NN package</a> contains many useful classes for creating computation graphs.\n",
        "<ul>\n",
        "<li> The class <a href=\"http://pytorch.org/docs/master/nn.html#module\">torch.nn.Module</a>: \n",
        "any new module must inherit from this class or its descendants (subclasses).\n",
        "</li>   \n",
        "<li> The `forward` method:  any class defining a module must implement the `forward(...)` method, which defines the transformation of inputs to outputs.</li>  \n",
        "<li> The class <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Linear\">`torch.nn.Linear(in_features, out_features)`</a>: this class implements a linear transformation. By default, it takes two parameters: \n",
        "    <ul>\n",
        "    <li>`in_features`: the size of the data at the input of the module. </li>\n",
        "    <li>`out_features`: the size of the data at the output of the module. </li>\n",
        "    </ul>\n",
        "</li>\n",
        "<li> The module <a href=\"http://pytorch.org/docs/master/nn.html#torch-nn-functional\">`torch.nn.functional`</a>: \n",
        "it defines a set of functions that can be applied directly to any tensor. As examples, we have:\n",
        "    <ul>\n",
        "    <li> non-linear functions: sigmoid(...), tanh(...), relu(...), etc...</li> \n",
        "    <li> cost functions: mse_loss(...), nll(...., cross_entropy(...), etc ... </li> \n",
        "    <li> regularization functions: droupout(...), etc ... </li> \n",
        "    <li> ...</li> \n",
        "    </ul>\n",
        "</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tscha6S-KIBB",
        "colab_type": "text"
      },
      "source": [
        "You need to complete the following methods:\n",
        "<ul>\n",
        "<li>The `__init__` method that defines the layers. </li>\n",
        "<li>The `forward(input)` method that returns the `output`. </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8NyQGwC-avkw",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "seed = 1234\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xR5eBfIbavk0",
        "colab": {}
      },
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(12, 20)\n",
        "        self.fc2 = nn.Linear(20, 40)\n",
        "        self.fc3 = nn.Linear(40, 20)\n",
        "        self.fc4 = nn.Linear(20, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        out = self.fc4(x)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OvLnHRZ5avk2"
      },
      "source": [
        "## Making predictions with a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uEXgJMDDavk3"
      },
      "source": [
        "Now, we are ready to test our neural network on randomly selected data.\n",
        "\n",
        "In PyTorch, a model has two different modes:\n",
        "    <ul>\n",
        "    <li> <b>train</b>: used during training, </li>\n",
        "    <li> <b>eval</b>: used during inference for model evaluation. </li>\n",
        "    </ul>\n",
        "The distinction is important since some modules behave differently according to this mode.\n",
        "We will use the <b>eval</b> mode in this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gzcABMezavk6",
        "outputId": "4c88d9b9-3469-4e8d-cccf-aec508316f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Model definition\n",
        "neural_net = NeuralNet()\n",
        "neural_net = neural_net.to(device)\n",
        "\n",
        "# Eval mode activation\n",
        "neural_net = neural_net.eval()\n",
        "\n",
        "# Select the first 5 data points\n",
        "data, target = val_dataset[0:5]\n",
        "data = data.to(device)\n",
        "target = target.to(device)\n",
        "\n",
        "# Forward propagation of the data through the model\n",
        "output = neural_net(data)   # equivalent to neural_net.forward(data)\n",
        "\n",
        "# Convert the logits into probabilities with softmax function\n",
        "m = nn.Softmax(dim=1)\n",
        "output_proba = m(output)\n",
        "\n",
        "# Printing the probability\n",
        "print(output_proba)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.5093, 0.4907],\n",
            "        [0.5069, 0.4931],\n",
            "        [0.5129, 0.4871],\n",
            "        [0.5022, 0.4978],\n",
            "        [0.4427, 0.5573]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fVep0BElavlS"
      },
      "source": [
        "The rows define the output of the network, in terms of probabilities over two classes: <b>deceased</b> (first column) or <b>survived</b> (second column), for each of the five input data points. Let us take the label with maximum probability as the predicted label and compare it to the correct label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_jV4No36qjdU",
        "outputId": "fc27df3f-e0c3-4f87-947b-d023e7113b5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Printing predictions (class with the highest probability)\n",
        "_, prediction = torch.max(output_proba, dim=1)\n",
        "\n",
        "print('Model prediction')\n",
        "print(prediction)\n",
        "\n",
        "# Printing the real labels\n",
        "print(\"Actual data\")\n",
        "print(target)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model prediction\n",
            "tensor([0, 0, 0, 0, 1], device='cuda:0')\n",
            "Actual data\n",
            "tensor([0, 0, 0, 0, 0], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SEIIjqOuqjdc"
      },
      "source": [
        "**Questions**\n",
        "\n",
        "1.   What would be a good way to measure the model performances?\n",
        "2.   How does our model perform?\n",
        "3.   Considering that the model is not trained on the dataset, do you see any problem with your selected measure?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTTlBikYwb-w",
        "colab_type": "text"
      },
      "source": [
        "1. Cross-entropy loss\n",
        "2. Poorly.  None of the model predictions are correct.\n",
        "3. Not really."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0uySA2TCavmD"
      },
      "source": [
        "## Define the cost function and optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EkoobCLMavmE"
      },
      "source": [
        "### Cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qkX7uSXQavmF"
      },
      "source": [
        "We define the cost function according to the task we want to achieve.\n",
        "\n",
        "PyTorch offers <a href=\"https://pytorch.org/docs/stable/nn.html#loss-functions\">many ready-to-use cost functions</a>.\n",
        "\n",
        "For classification problems, the usual cost function is <b>cross-entropy</b>, and this is the one we will use in this tutorial. In PyTorch, it is defined by the function <a href=\"http://pytorch.org/docs/master/nn.html#cross-entropy\">`torch.nn.functional.cross_entropy`</a>.  Cross entropy allows comparing a $p$ distribution with a reference distribution $t$. It attains its minimum when $t=p$. Its formula for calculating it between the prediction and the target is: $-\\sum_j t_{ij} \\log(p_{ij})$ where $p$ is the prediction, $t$ the target, $i$ the examples and $j$ the classes of the target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FHnfYeS5avmF",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def cost_function(prediction, target):\n",
        "    loss = F.cross_entropy(prediction, target)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vsx_cv9Wqjdj"
      },
      "source": [
        "### Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0hcZaIKtavmH"
      },
      "source": [
        "In Pytorch, thanks to the automatic differentiation mechanism <a href=\"http://pytorch.org/docs/master/notes/autograd.html\">Autograd</a>, it is possible to automatically calculate the gradient of the cost function and backpropagate it through the computational graph.\n",
        "\n",
        "To do this, we only have to call the method `backward()` on the variable returned by the cost function, e.g., with\n",
        "\n",
        "`loss = cost_function(....)` <br>\n",
        "`loss.backward()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8YNo_ymYavmH"
      },
      "source": [
        "### Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y4AlX9TwavmH"
      },
      "source": [
        "PyTorch provides a <a href=\"http://pytorch.org/docs/master/optim.html#algorithms\">set of optimization methods (`torch.optim`)</a> commonly used by the deep learning community. These methods include the following: \n",
        "<ul>\n",
        "<li><b>SGD</b> (Stochastic Gradient Descent) <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD(net.parameters(), lr=learning_rate)`</a></li>\n",
        "<li><b>Adam</b> (Adaptive Moment Estimation): a variant of the gradient descent method in which the learning rate is adjusted for each parameter. This adjustment is based on the estimation of the first and second moments of the gradients. This optimizer has demonstrated excellent performance compared to SGD on many benchmarks. </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Uam-a0_0qjdl"
      },
      "source": [
        "To be able to use an optimizer in PyTorch, we must instantiate it by passing the following elements:\n",
        "<ul>\n",
        "<li><b>The parameters of the model</b>: these are obtained using the method <b>parameters()</b> on the instantiated model. </li>\n",
        "<li><b>The learning rate (lr)</b>: this is the learning rate to be used to update parameters during the optimization process. </li>\n",
        "<li>There may be other parameters specific to the chosen optimizer.</li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jt6_Qr6ravmI"
      },
      "source": [
        "PyTorch offers a simplified interface to interact with any optimizer:\n",
        "<ul>\n",
        "<li><b>zero_grad()</b>: Allows to reinitialize the gradients to zero at the beginning of an optimization step.</li>\n",
        "<li><b>step()</b>: Allows to perform an optimization step after a gradient backpropagation step.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fZ-lKExqavmI"
      },
      "source": [
        "We will use Adam with a lr of 0.001."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WDMOziJTavmI",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(neural_net.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OnFOAfdGqjdr"
      },
      "source": [
        "# How to train a model?\n",
        "First, we need some definitions:\n",
        "<ol>\n",
        "<li>\n",
        "<b>Epoch</b>: a complete pass over the entire training dataset.\n",
        "</li>\n",
        "<li>\n",
        "<b>Iteration</b>: an update of the model parameters. Many iterations can occur before the end of an epoch.\n",
        "</li>\n",
        "<li>\n",
        "<b>Mini-batch</b>: A subset of training data used to estimate the average of gradients. In other words, at each iteration, a mini-batch is used. \n",
        "</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LLXjNiDTavmK"
      },
      "source": [
        "## Creating the mini-batches\n",
        "PyTorch offers a utility called <b><a href=\"http://pytorch.org/docs/master/data.html\"> torch.utils.data.DataLoader </a></b> to load any dataset and automatically split it into mini-batches. During training, the data presented to the network should appear in a different order from one epoch to another. We will prepare the `DataLoader` for our three datasets (training, validation, and test)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RGoQZSdqavmM",
        "colab": {}
      },
      "source": [
        "train_batch_size = 32  # number of data in a training batch.\n",
        "eval_batch_size = 32   # number of data in an batch.\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "\n",
        "val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=eval_batch_size, shuffle=True)\n",
        "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=eval_batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ia3ai-GvavmP"
      },
      "source": [
        "## Simple training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v9wNZrTnavmQ"
      },
      "source": [
        "Here we define our training procedure for an epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZyK9xCsZavmR",
        "colab": {}
      },
      "source": [
        "def train(epoch, model, train_loader, optimizer, device):\n",
        "    \n",
        "    # activate the training mode\n",
        "    model.train()\n",
        "    \n",
        "    torch.set_grad_enabled(True)\n",
        "    \n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    # iteration over the mini-batches\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        \n",
        "        # transfer the data on the chosen device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # reinitialize the gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward propagation on the data\n",
        "        prediction = model(data)\n",
        "        \n",
        "        # compute the cost function w.r.t. the targets\n",
        "        loss = cost_function(prediction, target)\n",
        "        \n",
        "        # execute the backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # execute an optimization step\n",
        "        optimizer.step()\n",
        "        \n",
        "        # accumulate the loss\n",
        "        total_loss += loss.item()*len(data)\n",
        "        \n",
        "        # compute the number of correct predictions\n",
        "        _, pred_classes = torch.max(prediction, dim=1)        \n",
        "        correct += pred_classes.eq(target.view_as(pred_classes)).sum().item()\n",
        "         \n",
        "        \n",
        "    # compute the average cost per epoch\n",
        "    mean_loss = total_loss/len(train_loader.dataset)\n",
        "    \n",
        "    # compute the accuracy\n",
        "    acc = correct / len(train_loader.dataset)\n",
        "        \n",
        "    print('Train Epoch: {}   Avg_Loss: {:.5f}   Acc: {}/{} ({:.3f}%)'.format(\n",
        "        epoch, mean_loss, correct, len(train_loader.dataset),\n",
        "        100. * acc))   \n",
        "    \n",
        "    # return the average loss and the accuracy\n",
        "    return mean_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PxG666rmavmU"
      },
      "source": [
        "## Evaluation procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vGexbWaHavmU"
      },
      "source": [
        "Here we define our model evaluation procedure.\n",
        "<br/>\n",
        "In addition to switching the model to **eval** mode, it is essential to disable the gradient calculation. \n",
        "<br/>\n",
        "To do this, PyTorch offers a set of context managers to <a href=\"https://pytorch.org/docs/0.4.0/torch.html#locally-disabling-gradient-computation\">locally disable/enable gradient calculation </a>:\n",
        "<ol>\n",
        "<li>\n",
        "`torch.no_grad()`: disable gradient calculation.\n",
        "</li>\n",
        "<li>\n",
        "`torch.enable_grad()`: enable gradient calculation.\n",
        "</li>\n",
        "<li>\n",
        "`torch.set_grad_enabled(bool)`: enable/disable gradient calculation.\n",
        "</li>\n",
        "</ol>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8gQj9W5LavmU",
        "colab": {}
      },
      "source": [
        "def evaluate(model, eval_loader, device):\n",
        "    \n",
        "    # activate the evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        # iterate over the batches\n",
        "        for batch_idx, (data, target) in enumerate(eval_loader):\n",
        "\n",
        "            # transfer the data on the chosen device\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # forward propagation on the data\n",
        "            prediction = model(data)\n",
        "\n",
        "            # compute the cost function w.r.t. the targets\n",
        "            loss = cost_function(prediction, target)           \n",
        "\n",
        "\n",
        "            # accumulate the loss\n",
        "            total_loss += loss.item()*len(data)\n",
        "\n",
        "            # compute the number of correct predictions en sortie)\n",
        "            _, pred_classes = torch.max(prediction, dim=1) \n",
        "            correct += pred_classes.eq(target.view_as(pred_classes)).sum().item()         \n",
        "          \n",
        "    \n",
        "    # compute the average cost per epoch\n",
        "    mean_loss = total_loss/len(eval_loader.dataset)\n",
        "    \n",
        "    # compute the accuracy\n",
        "    acc = correct / len(eval_loader.dataset)\n",
        "        \n",
        "    print('Eval:  Avg_Loss: {:.5f}   Acc: {}/{} ({:.3f}%)'.format(\n",
        "        mean_loss, correct, len(eval_loader.dataset),\n",
        "        100. * acc)) \n",
        "    \n",
        "    # return the average loss and the accuracy\n",
        "    return mean_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fMUyZNxdavmW"
      },
      "source": [
        "## Checkpointing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lQLklQXAavmW"
      },
      "source": [
        "For training phases that require much time, it is recommended to save periodically the model parameters. This step is commonly referred to as <b>checkpointing</b>.\n",
        "\n",
        "PyTorch offers <a href=\"http://pytorch.org/docs/master/notes/serialization.html\">a simple mechanism</a> to perform this operation. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ld-Y2gF-avmX"
      },
      "source": [
        "We implement two methods here:\n",
        "<ul>\n",
        "<li> the first one for <b> saving </b> a model,</li>\n",
        "<li> the second for <b> loading </b> a model checkpoint. </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dMmNpma2avmX",
        "colab": {}
      },
      "source": [
        "def save_model(epoch, model, path='./'):\n",
        "    \n",
        "    # creating the file name indexed by the epoch value\n",
        "    filename = path + 'neural_network_{}.pt'.format(epoch)\n",
        "    \n",
        "    # saving the model parameters\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    \n",
        "    \n",
        "    return model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ZptgqQRavmZ",
        "colab": {}
      },
      "source": [
        "def load_model(epoch, model, path='./'):\n",
        "    \n",
        "    # creating the file name indexed by the epoch value\n",
        "    filename = path + 'neural_network_{}.pt'.format(epoch)\n",
        "    \n",
        "    # loading the parameters of the saved model\n",
        "    model.load_state_dict(torch.load(filename))\n",
        "    \n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ve8sOocWavma"
      },
      "source": [
        "It is also possible to save the status of the optimizer in PyTorch, which is very important when we want to resume training the model from a given backup. For more information, please consult <a href='https://discuss.pytorch.org/t/saving-and-loading-a-model-in-pytorch/2610/3'>the following URL</a>. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8lcAP8-1avma"
      },
      "source": [
        "## Putting everything together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "keMpyePsavmb",
        "outputId": "131fcefb-255a-42a4-c6ee-3733f237306b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# maximum number of epoch\n",
        "numEpochs = 200\n",
        "\n",
        "# Saving frequency\n",
        "checkpoint_freq = 10\n",
        "\n",
        "# Directory for data backup\n",
        "path = './'\n",
        "\n",
        "# Accumulators of average costs obtained per epoch\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Performance accumulators per epoch\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Model definition\n",
        "neural_net = NeuralNet()\n",
        "\n",
        "# Load the model on the chosen device\n",
        "neural_net = neural_net.to(device)\n",
        "\n",
        "# Optimizer definition\n",
        "optimizer = optim.Adam(neural_net.parameters(), lr=0.001) \n",
        "# optimizer = optim.SGD(neural_net.parameters(), lr=0.001) \n",
        "\n",
        "\n",
        "# Learning loop\n",
        "for epoch in range(1, numEpochs + 1):\n",
        "    \n",
        "    # train the model with the train dataset\n",
        "    train_loss, train_acc = train(epoch, neural_net, train_loader, optimizer, device)   \n",
        "    \n",
        "    # evaluate the model with the validation dataset\n",
        "    val_loss, val_acc = evaluate(neural_net, val_loader, device)       \n",
        "    \n",
        "    # Save the costs obtained\n",
        "    train_losses.append(train_loss)    \n",
        "    val_losses.append(val_loss)\n",
        "    \n",
        "    # Save the performances\n",
        "    train_accuracies.append(train_acc)    \n",
        "    val_accuracies.append(val_acc)\n",
        "    \n",
        "    # Checkpoint\n",
        "    if epoch % checkpoint_freq ==0:\n",
        "        save_model(epoch, neural_net, path)\n",
        "\n",
        "# Save the model at the end of the training\n",
        "save_model(numEpochs, neural_net, path)\n",
        "    \n",
        "print(\"\\n\\n\\nOptimization ended.\\n\")    \n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1   Avg_Loss: 0.82669   Acc: 275/625 (44.000%)\n",
            "Eval:  Avg_Loss: 0.69597   Acc: 99/209 (47.368%)\n",
            "Train Epoch: 2   Avg_Loss: 0.65850   Acc: 406/625 (64.960%)\n",
            "Eval:  Avg_Loss: 0.64826   Acc: 143/209 (68.421%)\n",
            "Train Epoch: 3   Avg_Loss: 0.63012   Acc: 422/625 (67.520%)\n",
            "Eval:  Avg_Loss: 0.62541   Acc: 144/209 (68.900%)\n",
            "Train Epoch: 4   Avg_Loss: 0.61821   Acc: 424/625 (67.840%)\n",
            "Eval:  Avg_Loss: 0.61376   Acc: 142/209 (67.943%)\n",
            "Train Epoch: 5   Avg_Loss: 0.60985   Acc: 424/625 (67.840%)\n",
            "Eval:  Avg_Loss: 0.60510   Acc: 142/209 (67.943%)\n",
            "Train Epoch: 6   Avg_Loss: 0.60632   Acc: 428/625 (68.480%)\n",
            "Eval:  Avg_Loss: 0.59929   Acc: 144/209 (68.900%)\n",
            "Train Epoch: 7   Avg_Loss: 0.60377   Acc: 421/625 (67.360%)\n",
            "Eval:  Avg_Loss: 0.59803   Acc: 144/209 (68.900%)\n",
            "Train Epoch: 8   Avg_Loss: 0.59569   Acc: 427/625 (68.320%)\n",
            "Eval:  Avg_Loss: 0.58945   Acc: 145/209 (69.378%)\n",
            "Train Epoch: 9   Avg_Loss: 0.59309   Acc: 430/625 (68.800%)\n",
            "Eval:  Avg_Loss: 0.58104   Acc: 146/209 (69.856%)\n",
            "Train Epoch: 10   Avg_Loss: 0.59179   Acc: 435/625 (69.600%)\n",
            "Eval:  Avg_Loss: 0.57680   Acc: 145/209 (69.378%)\n",
            "Train Epoch: 11   Avg_Loss: 0.57512   Acc: 433/625 (69.280%)\n",
            "Eval:  Avg_Loss: 0.56874   Acc: 145/209 (69.378%)\n",
            "Train Epoch: 12   Avg_Loss: 0.56806   Acc: 441/625 (70.560%)\n",
            "Eval:  Avg_Loss: 0.55840   Acc: 147/209 (70.335%)\n",
            "Train Epoch: 13   Avg_Loss: 0.56444   Acc: 457/625 (73.120%)\n",
            "Eval:  Avg_Loss: 0.55418   Acc: 147/209 (70.335%)\n",
            "Train Epoch: 14   Avg_Loss: 0.54458   Acc: 462/625 (73.920%)\n",
            "Eval:  Avg_Loss: 0.53808   Acc: 153/209 (73.206%)\n",
            "Train Epoch: 15   Avg_Loss: 0.53177   Acc: 469/625 (75.040%)\n",
            "Eval:  Avg_Loss: 0.53186   Acc: 149/209 (71.292%)\n",
            "Train Epoch: 16   Avg_Loss: 0.51949   Acc: 475/625 (76.000%)\n",
            "Eval:  Avg_Loss: 0.51475   Acc: 160/209 (76.555%)\n",
            "Train Epoch: 17   Avg_Loss: 0.50293   Acc: 480/625 (76.800%)\n",
            "Eval:  Avg_Loss: 0.51061   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 18   Avg_Loss: 0.48700   Acc: 491/625 (78.560%)\n",
            "Eval:  Avg_Loss: 0.51636   Acc: 162/209 (77.512%)\n",
            "Train Epoch: 19   Avg_Loss: 0.48710   Acc: 495/625 (79.200%)\n",
            "Eval:  Avg_Loss: 0.49775   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 20   Avg_Loss: 0.48913   Acc: 488/625 (78.080%)\n",
            "Eval:  Avg_Loss: 0.51263   Acc: 159/209 (76.077%)\n",
            "Train Epoch: 21   Avg_Loss: 0.46846   Acc: 492/625 (78.720%)\n",
            "Eval:  Avg_Loss: 0.50197   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 22   Avg_Loss: 0.45553   Acc: 500/625 (80.000%)\n",
            "Eval:  Avg_Loss: 0.49613   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 23   Avg_Loss: 0.45452   Acc: 495/625 (79.200%)\n",
            "Eval:  Avg_Loss: 0.50118   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 24   Avg_Loss: 0.45607   Acc: 498/625 (79.680%)\n",
            "Eval:  Avg_Loss: 0.51247   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 25   Avg_Loss: 0.45085   Acc: 499/625 (79.840%)\n",
            "Eval:  Avg_Loss: 0.50153   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 26   Avg_Loss: 0.45898   Acc: 494/625 (79.040%)\n",
            "Eval:  Avg_Loss: 0.51583   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 27   Avg_Loss: 0.45134   Acc: 502/625 (80.320%)\n",
            "Eval:  Avg_Loss: 0.50915   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 28   Avg_Loss: 0.44525   Acc: 500/625 (80.000%)\n",
            "Eval:  Avg_Loss: 0.50253   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 29   Avg_Loss: 0.44426   Acc: 500/625 (80.000%)\n",
            "Eval:  Avg_Loss: 0.51190   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 30   Avg_Loss: 0.46858   Acc: 492/625 (78.720%)\n",
            "Eval:  Avg_Loss: 0.52152   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 31   Avg_Loss: 0.44253   Acc: 507/625 (81.120%)\n",
            "Eval:  Avg_Loss: 0.51773   Acc: 159/209 (76.077%)\n",
            "Train Epoch: 32   Avg_Loss: 0.43517   Acc: 504/625 (80.640%)\n",
            "Eval:  Avg_Loss: 0.51228   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 33   Avg_Loss: 0.43485   Acc: 506/625 (80.960%)\n",
            "Eval:  Avg_Loss: 0.50064   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 34   Avg_Loss: 0.43151   Acc: 510/625 (81.600%)\n",
            "Eval:  Avg_Loss: 0.50669   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 35   Avg_Loss: 0.43067   Acc: 503/625 (80.480%)\n",
            "Eval:  Avg_Loss: 0.51438   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 36   Avg_Loss: 0.43007   Acc: 508/625 (81.280%)\n",
            "Eval:  Avg_Loss: 0.50259   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 37   Avg_Loss: 0.42670   Acc: 505/625 (80.800%)\n",
            "Eval:  Avg_Loss: 0.50689   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 38   Avg_Loss: 0.42275   Acc: 511/625 (81.760%)\n",
            "Eval:  Avg_Loss: 0.50945   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 39   Avg_Loss: 0.42285   Acc: 507/625 (81.120%)\n",
            "Eval:  Avg_Loss: 0.52366   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 40   Avg_Loss: 0.42955   Acc: 502/625 (80.320%)\n",
            "Eval:  Avg_Loss: 0.51632   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 41   Avg_Loss: 0.42867   Acc: 509/625 (81.440%)\n",
            "Eval:  Avg_Loss: 0.51964   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 42   Avg_Loss: 0.41779   Acc: 505/625 (80.800%)\n",
            "Eval:  Avg_Loss: 0.50866   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 43   Avg_Loss: 0.41841   Acc: 508/625 (81.280%)\n",
            "Eval:  Avg_Loss: 0.51318   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 44   Avg_Loss: 0.42109   Acc: 509/625 (81.440%)\n",
            "Eval:  Avg_Loss: 0.51571   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 45   Avg_Loss: 0.42191   Acc: 507/625 (81.120%)\n",
            "Eval:  Avg_Loss: 0.51831   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 46   Avg_Loss: 0.41952   Acc: 507/625 (81.120%)\n",
            "Eval:  Avg_Loss: 0.53006   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 47   Avg_Loss: 0.42187   Acc: 507/625 (81.120%)\n",
            "Eval:  Avg_Loss: 0.51556   Acc: 162/209 (77.512%)\n",
            "Train Epoch: 48   Avg_Loss: 0.42089   Acc: 511/625 (81.760%)\n",
            "Eval:  Avg_Loss: 0.52196   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 49   Avg_Loss: 0.42243   Acc: 504/625 (80.640%)\n",
            "Eval:  Avg_Loss: 0.53337   Acc: 161/209 (77.033%)\n",
            "Train Epoch: 50   Avg_Loss: 0.44248   Acc: 496/625 (79.360%)\n",
            "Eval:  Avg_Loss: 0.56288   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 51   Avg_Loss: 0.41574   Acc: 509/625 (81.440%)\n",
            "Eval:  Avg_Loss: 0.50131   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 52   Avg_Loss: 0.40837   Acc: 513/625 (82.080%)\n",
            "Eval:  Avg_Loss: 0.52545   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 53   Avg_Loss: 0.41292   Acc: 512/625 (81.920%)\n",
            "Eval:  Avg_Loss: 0.53047   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 54   Avg_Loss: 0.41307   Acc: 515/625 (82.400%)\n",
            "Eval:  Avg_Loss: 0.52885   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 55   Avg_Loss: 0.41528   Acc: 508/625 (81.280%)\n",
            "Eval:  Avg_Loss: 0.50485   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 56   Avg_Loss: 0.41314   Acc: 508/625 (81.280%)\n",
            "Eval:  Avg_Loss: 0.54372   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 57   Avg_Loss: 0.42676   Acc: 505/625 (80.800%)\n",
            "Eval:  Avg_Loss: 0.55514   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 58   Avg_Loss: 0.43494   Acc: 505/625 (80.800%)\n",
            "Eval:  Avg_Loss: 0.54931   Acc: 161/209 (77.033%)\n",
            "Train Epoch: 59   Avg_Loss: 0.41116   Acc: 515/625 (82.400%)\n",
            "Eval:  Avg_Loss: 0.54184   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 60   Avg_Loss: 0.41024   Acc: 512/625 (81.920%)\n",
            "Eval:  Avg_Loss: 0.53018   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 61   Avg_Loss: 0.40700   Acc: 513/625 (82.080%)\n",
            "Eval:  Avg_Loss: 0.53357   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 62   Avg_Loss: 0.40700   Acc: 511/625 (81.760%)\n",
            "Eval:  Avg_Loss: 0.53117   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 63   Avg_Loss: 0.41135   Acc: 512/625 (81.920%)\n",
            "Eval:  Avg_Loss: 0.56039   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 64   Avg_Loss: 0.40381   Acc: 515/625 (82.400%)\n",
            "Eval:  Avg_Loss: 0.51949   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 65   Avg_Loss: 0.40118   Acc: 512/625 (81.920%)\n",
            "Eval:  Avg_Loss: 0.52470   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 66   Avg_Loss: 0.40018   Acc: 512/625 (81.920%)\n",
            "Eval:  Avg_Loss: 0.54634   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 67   Avg_Loss: 0.40575   Acc: 509/625 (81.440%)\n",
            "Eval:  Avg_Loss: 0.53385   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 68   Avg_Loss: 0.40737   Acc: 510/625 (81.600%)\n",
            "Eval:  Avg_Loss: 0.51464   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 69   Avg_Loss: 0.40644   Acc: 512/625 (81.920%)\n",
            "Eval:  Avg_Loss: 0.53307   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 70   Avg_Loss: 0.39418   Acc: 516/625 (82.560%)\n",
            "Eval:  Avg_Loss: 0.54031   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 71   Avg_Loss: 0.39612   Acc: 517/625 (82.720%)\n",
            "Eval:  Avg_Loss: 0.53105   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 72   Avg_Loss: 0.39864   Acc: 516/625 (82.560%)\n",
            "Eval:  Avg_Loss: 0.53935   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 73   Avg_Loss: 0.39616   Acc: 514/625 (82.240%)\n",
            "Eval:  Avg_Loss: 0.53970   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 74   Avg_Loss: 0.39667   Acc: 517/625 (82.720%)\n",
            "Eval:  Avg_Loss: 0.55711   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 75   Avg_Loss: 0.41143   Acc: 513/625 (82.080%)\n",
            "Eval:  Avg_Loss: 0.53077   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 76   Avg_Loss: 0.41532   Acc: 513/625 (82.080%)\n",
            "Eval:  Avg_Loss: 0.53454   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 77   Avg_Loss: 0.39868   Acc: 517/625 (82.720%)\n",
            "Eval:  Avg_Loss: 0.54773   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 78   Avg_Loss: 0.39264   Acc: 522/625 (83.520%)\n",
            "Eval:  Avg_Loss: 0.56033   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 79   Avg_Loss: 0.39597   Acc: 515/625 (82.400%)\n",
            "Eval:  Avg_Loss: 0.56441   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 80   Avg_Loss: 0.39843   Acc: 520/625 (83.200%)\n",
            "Eval:  Avg_Loss: 0.55200   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 81   Avg_Loss: 0.38947   Acc: 520/625 (83.200%)\n",
            "Eval:  Avg_Loss: 0.55720   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 82   Avg_Loss: 0.39010   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.54893   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 83   Avg_Loss: 0.39460   Acc: 521/625 (83.360%)\n",
            "Eval:  Avg_Loss: 0.54867   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 84   Avg_Loss: 0.39307   Acc: 517/625 (82.720%)\n",
            "Eval:  Avg_Loss: 0.55812   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 85   Avg_Loss: 0.39425   Acc: 516/625 (82.560%)\n",
            "Eval:  Avg_Loss: 0.58134   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 86   Avg_Loss: 0.38823   Acc: 520/625 (83.200%)\n",
            "Eval:  Avg_Loss: 0.58528   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 87   Avg_Loss: 0.38989   Acc: 520/625 (83.200%)\n",
            "Eval:  Avg_Loss: 0.56877   Acc: 172/209 (82.297%)\n",
            "Train Epoch: 88   Avg_Loss: 0.39032   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.56349   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 89   Avg_Loss: 0.38577   Acc: 517/625 (82.720%)\n",
            "Eval:  Avg_Loss: 0.56681   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 90   Avg_Loss: 0.39089   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.55070   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 91   Avg_Loss: 0.39630   Acc: 528/625 (84.480%)\n",
            "Eval:  Avg_Loss: 0.56445   Acc: 173/209 (82.775%)\n",
            "Train Epoch: 92   Avg_Loss: 0.40375   Acc: 515/625 (82.400%)\n",
            "Eval:  Avg_Loss: 0.57946   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 93   Avg_Loss: 0.39637   Acc: 518/625 (82.880%)\n",
            "Eval:  Avg_Loss: 0.54121   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 94   Avg_Loss: 0.38739   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.57454   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 95   Avg_Loss: 0.38669   Acc: 520/625 (83.200%)\n",
            "Eval:  Avg_Loss: 0.56038   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 96   Avg_Loss: 0.38483   Acc: 520/625 (83.200%)\n",
            "Eval:  Avg_Loss: 0.58535   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 97   Avg_Loss: 0.39275   Acc: 520/625 (83.200%)\n",
            "Eval:  Avg_Loss: 0.57416   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 98   Avg_Loss: 0.39067   Acc: 518/625 (82.880%)\n",
            "Eval:  Avg_Loss: 0.56240   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 99   Avg_Loss: 0.38647   Acc: 515/625 (82.400%)\n",
            "Eval:  Avg_Loss: 0.57478   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 100   Avg_Loss: 0.38339   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.58290   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 101   Avg_Loss: 0.38813   Acc: 520/625 (83.200%)\n",
            "Eval:  Avg_Loss: 0.55882   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 102   Avg_Loss: 0.39123   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.57525   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 103   Avg_Loss: 0.38311   Acc: 517/625 (82.720%)\n",
            "Eval:  Avg_Loss: 0.57953   Acc: 172/209 (82.297%)\n",
            "Train Epoch: 104   Avg_Loss: 0.38776   Acc: 515/625 (82.400%)\n",
            "Eval:  Avg_Loss: 0.57695   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 105   Avg_Loss: 0.39583   Acc: 508/625 (81.280%)\n",
            "Eval:  Avg_Loss: 0.60303   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 106   Avg_Loss: 0.38372   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.55122   Acc: 172/209 (82.297%)\n",
            "Train Epoch: 107   Avg_Loss: 0.38114   Acc: 520/625 (83.200%)\n",
            "Eval:  Avg_Loss: 0.58337   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 108   Avg_Loss: 0.37844   Acc: 522/625 (83.520%)\n",
            "Eval:  Avg_Loss: 0.57823   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 109   Avg_Loss: 0.38427   Acc: 514/625 (82.240%)\n",
            "Eval:  Avg_Loss: 0.59792   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 110   Avg_Loss: 0.38541   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.59982   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 111   Avg_Loss: 0.39053   Acc: 516/625 (82.560%)\n",
            "Eval:  Avg_Loss: 0.68024   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 112   Avg_Loss: 0.40546   Acc: 509/625 (81.440%)\n",
            "Eval:  Avg_Loss: 0.61509   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 113   Avg_Loss: 0.39166   Acc: 509/625 (81.440%)\n",
            "Eval:  Avg_Loss: 0.53852   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 114   Avg_Loss: 0.38556   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.56821   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 115   Avg_Loss: 0.39027   Acc: 516/625 (82.560%)\n",
            "Eval:  Avg_Loss: 0.58639   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 116   Avg_Loss: 0.38667   Acc: 517/625 (82.720%)\n",
            "Eval:  Avg_Loss: 0.58722   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 117   Avg_Loss: 0.38339   Acc: 525/625 (84.000%)\n",
            "Eval:  Avg_Loss: 0.59736   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 118   Avg_Loss: 0.39209   Acc: 513/625 (82.080%)\n",
            "Eval:  Avg_Loss: 0.58087   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 119   Avg_Loss: 0.38412   Acc: 515/625 (82.400%)\n",
            "Eval:  Avg_Loss: 0.57748   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 120   Avg_Loss: 0.38064   Acc: 516/625 (82.560%)\n",
            "Eval:  Avg_Loss: 0.57599   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 121   Avg_Loss: 0.37172   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.59729   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 122   Avg_Loss: 0.37483   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.58926   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 123   Avg_Loss: 0.39617   Acc: 513/625 (82.080%)\n",
            "Eval:  Avg_Loss: 0.59025   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 124   Avg_Loss: 0.38345   Acc: 521/625 (83.360%)\n",
            "Eval:  Avg_Loss: 0.57120   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 125   Avg_Loss: 0.38544   Acc: 513/625 (82.080%)\n",
            "Eval:  Avg_Loss: 0.59829   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 126   Avg_Loss: 0.37599   Acc: 518/625 (82.880%)\n",
            "Eval:  Avg_Loss: 0.52870   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 127   Avg_Loss: 0.40661   Acc: 507/625 (81.120%)\n",
            "Eval:  Avg_Loss: 0.54564   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 128   Avg_Loss: 0.37960   Acc: 520/625 (83.200%)\n",
            "Eval:  Avg_Loss: 0.55097   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 129   Avg_Loss: 0.37465   Acc: 527/625 (84.320%)\n",
            "Eval:  Avg_Loss: 0.54938   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 130   Avg_Loss: 0.37411   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.56420   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 131   Avg_Loss: 0.37132   Acc: 526/625 (84.160%)\n",
            "Eval:  Avg_Loss: 0.57814   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 132   Avg_Loss: 0.37009   Acc: 521/625 (83.360%)\n",
            "Eval:  Avg_Loss: 0.57471   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 133   Avg_Loss: 0.37313   Acc: 521/625 (83.360%)\n",
            "Eval:  Avg_Loss: 0.59222   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 134   Avg_Loss: 0.37603   Acc: 521/625 (83.360%)\n",
            "Eval:  Avg_Loss: 0.58411   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 135   Avg_Loss: 0.36635   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.59592   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 136   Avg_Loss: 0.37603   Acc: 515/625 (82.400%)\n",
            "Eval:  Avg_Loss: 0.61860   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 137   Avg_Loss: 0.37443   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.59732   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 138   Avg_Loss: 0.36906   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.57786   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 139   Avg_Loss: 0.36673   Acc: 521/625 (83.360%)\n",
            "Eval:  Avg_Loss: 0.58630   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 140   Avg_Loss: 0.36670   Acc: 522/625 (83.520%)\n",
            "Eval:  Avg_Loss: 0.60909   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 141   Avg_Loss: 0.37040   Acc: 525/625 (84.000%)\n",
            "Eval:  Avg_Loss: 0.58611   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 142   Avg_Loss: 0.36296   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.60049   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 143   Avg_Loss: 0.37042   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.58858   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 144   Avg_Loss: 0.38660   Acc: 513/625 (82.080%)\n",
            "Eval:  Avg_Loss: 0.60788   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 145   Avg_Loss: 0.37780   Acc: 525/625 (84.000%)\n",
            "Eval:  Avg_Loss: 0.59612   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 146   Avg_Loss: 0.37392   Acc: 521/625 (83.360%)\n",
            "Eval:  Avg_Loss: 0.60527   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 147   Avg_Loss: 0.38154   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.63769   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 148   Avg_Loss: 0.36252   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.62009   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 149   Avg_Loss: 0.38514   Acc: 517/625 (82.720%)\n",
            "Eval:  Avg_Loss: 0.63392   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 150   Avg_Loss: 0.39674   Acc: 518/625 (82.880%)\n",
            "Eval:  Avg_Loss: 0.67155   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 151   Avg_Loss: 0.40247   Acc: 515/625 (82.400%)\n",
            "Eval:  Avg_Loss: 0.58238   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 152   Avg_Loss: 0.37740   Acc: 525/625 (84.000%)\n",
            "Eval:  Avg_Loss: 0.58757   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 153   Avg_Loss: 0.36648   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.60152   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 154   Avg_Loss: 0.36480   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.59815   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 155   Avg_Loss: 0.35941   Acc: 526/625 (84.160%)\n",
            "Eval:  Avg_Loss: 0.60154   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 156   Avg_Loss: 0.36069   Acc: 520/625 (83.200%)\n",
            "Eval:  Avg_Loss: 0.59441   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 157   Avg_Loss: 0.36710   Acc: 527/625 (84.320%)\n",
            "Eval:  Avg_Loss: 0.59946   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 158   Avg_Loss: 0.36375   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.59847   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 159   Avg_Loss: 0.35975   Acc: 525/625 (84.000%)\n",
            "Eval:  Avg_Loss: 0.58816   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 160   Avg_Loss: 0.36000   Acc: 528/625 (84.480%)\n",
            "Eval:  Avg_Loss: 0.59744   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 161   Avg_Loss: 0.36312   Acc: 521/625 (83.360%)\n",
            "Eval:  Avg_Loss: 0.59201   Acc: 172/209 (82.297%)\n",
            "Train Epoch: 162   Avg_Loss: 0.36261   Acc: 517/625 (82.720%)\n",
            "Eval:  Avg_Loss: 0.59872   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 163   Avg_Loss: 0.36559   Acc: 525/625 (84.000%)\n",
            "Eval:  Avg_Loss: 0.60737   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 164   Avg_Loss: 0.36973   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.58029   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 165   Avg_Loss: 0.36457   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.59517   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 166   Avg_Loss: 0.35780   Acc: 522/625 (83.520%)\n",
            "Eval:  Avg_Loss: 0.59536   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 167   Avg_Loss: 0.36096   Acc: 526/625 (84.160%)\n",
            "Eval:  Avg_Loss: 0.60947   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 168   Avg_Loss: 0.35641   Acc: 528/625 (84.480%)\n",
            "Eval:  Avg_Loss: 0.61768   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 169   Avg_Loss: 0.36251   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.60014   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 170   Avg_Loss: 0.36212   Acc: 527/625 (84.320%)\n",
            "Eval:  Avg_Loss: 0.60772   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 171   Avg_Loss: 0.35608   Acc: 532/625 (85.120%)\n",
            "Eval:  Avg_Loss: 0.60450   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 172   Avg_Loss: 0.35380   Acc: 532/625 (85.120%)\n",
            "Eval:  Avg_Loss: 0.60896   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 173   Avg_Loss: 0.36016   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.62593   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 174   Avg_Loss: 0.35613   Acc: 528/625 (84.480%)\n",
            "Eval:  Avg_Loss: 0.61452   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 175   Avg_Loss: 0.35951   Acc: 526/625 (84.160%)\n",
            "Eval:  Avg_Loss: 0.63668   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 176   Avg_Loss: 0.37684   Acc: 509/625 (81.440%)\n",
            "Eval:  Avg_Loss: 0.61889   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 177   Avg_Loss: 0.36517   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.62542   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 178   Avg_Loss: 0.35424   Acc: 529/625 (84.640%)\n",
            "Eval:  Avg_Loss: 0.59715   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 179   Avg_Loss: 0.35112   Acc: 531/625 (84.960%)\n",
            "Eval:  Avg_Loss: 0.62278   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 180   Avg_Loss: 0.35200   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.59436   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 181   Avg_Loss: 0.36002   Acc: 528/625 (84.480%)\n",
            "Eval:  Avg_Loss: 0.59809   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 182   Avg_Loss: 0.36289   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.58937   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 183   Avg_Loss: 0.35571   Acc: 521/625 (83.360%)\n",
            "Eval:  Avg_Loss: 0.60816   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 184   Avg_Loss: 0.35333   Acc: 535/625 (85.600%)\n",
            "Eval:  Avg_Loss: 0.59561   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 185   Avg_Loss: 0.35458   Acc: 520/625 (83.200%)\n",
            "Eval:  Avg_Loss: 0.59300   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 186   Avg_Loss: 0.35033   Acc: 531/625 (84.960%)\n",
            "Eval:  Avg_Loss: 0.60366   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 187   Avg_Loss: 0.35393   Acc: 526/625 (84.160%)\n",
            "Eval:  Avg_Loss: 0.55982   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 188   Avg_Loss: 0.35341   Acc: 530/625 (84.800%)\n",
            "Eval:  Avg_Loss: 0.58753   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 189   Avg_Loss: 0.34838   Acc: 529/625 (84.640%)\n",
            "Eval:  Avg_Loss: 0.58573   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 190   Avg_Loss: 0.35059   Acc: 532/625 (85.120%)\n",
            "Eval:  Avg_Loss: 0.59138   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 191   Avg_Loss: 0.35516   Acc: 527/625 (84.320%)\n",
            "Eval:  Avg_Loss: 0.56706   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 192   Avg_Loss: 0.35072   Acc: 526/625 (84.160%)\n",
            "Eval:  Avg_Loss: 0.57513   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 193   Avg_Loss: 0.34925   Acc: 532/625 (85.120%)\n",
            "Eval:  Avg_Loss: 0.58946   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 194   Avg_Loss: 0.35330   Acc: 531/625 (84.960%)\n",
            "Eval:  Avg_Loss: 0.57370   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 195   Avg_Loss: 0.36728   Acc: 527/625 (84.320%)\n",
            "Eval:  Avg_Loss: 0.57346   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 196   Avg_Loss: 0.36395   Acc: 531/625 (84.960%)\n",
            "Eval:  Avg_Loss: 0.60287   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 197   Avg_Loss: 0.36077   Acc: 526/625 (84.160%)\n",
            "Eval:  Avg_Loss: 0.56235   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 198   Avg_Loss: 0.39852   Acc: 517/625 (82.720%)\n",
            "Eval:  Avg_Loss: 0.66387   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 199   Avg_Loss: 0.35891   Acc: 521/625 (83.360%)\n",
            "Eval:  Avg_Loss: 0.58412   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 200   Avg_Loss: 0.35215   Acc: 527/625 (84.320%)\n",
            "Eval:  Avg_Loss: 0.59707   Acc: 164/209 (78.469%)\n",
            "\n",
            "\n",
            "\n",
            "Optimization ended.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "86OZRLrjavmd"
      },
      "source": [
        "## Interpreting the output of the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mklvQruYavme",
        "outputId": "b680579d-380b-417a-e421-689c9a9a75fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Activate the evaluation mode\n",
        "neural_net = neural_net.eval()\n",
        "\n",
        "# Select the first 10 data points of the validation set\n",
        "data, target = val_dataset[0:10]\n",
        "data = data.to(device)\n",
        "\n",
        "# Executing the neural network\n",
        "output = neural_net(data)   # equivalent to neural_net.forward(data)\n",
        "\n",
        "# Transform the output into a probability distribution with a softmax function\n",
        "# m = nn.Softmax(dim=1)\n",
        "output_proba = m(output)\n",
        "\n",
        "# Print the probability\n",
        "print(output_proba)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.9302, 0.0698],\n",
            "        [0.5484, 0.4516],\n",
            "        [0.4975, 0.5025],\n",
            "        [0.5397, 0.4603],\n",
            "        [0.7329, 0.2671],\n",
            "        [0.8742, 0.1258],\n",
            "        [0.8404, 0.1596],\n",
            "        [0.7983, 0.2017],\n",
            "        [0.0110, 0.9890],\n",
            "        [0.0010, 0.9990]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RvIEqKt0qjeT",
        "outputId": "e60101d3-ff64-4764-c513-cdc110c2e612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# For each example, retrieve the class with the highest probability.\n",
        "_, prediction = torch.max(output_proba, dim=1)\n",
        "\n",
        "print(\"Model predictions\")\n",
        "print(prediction)\n",
        "\n",
        "print(\"Targets\")\n",
        "print(target)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model predictions\n",
            "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "Targets\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V11J3Jihavmy"
      },
      "source": [
        "## Visualizing of the learning curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j9_9C_tXavmz"
      },
      "source": [
        "The visualization of the learning curve allows to detect possible problems that may have occurred during learning, for example, overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iNcbpl0tavm0",
        "outputId": "f89d3f39-2827-4a3b-a340-e9952b1f95a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "x = list(range(len(train_losses)))\n",
        "\n",
        "ax = plt.subplot(111)\n",
        "plt.plot(x, train_losses, 'r', label=\"Train\")\n",
        "plt.plot(x, val_losses, 'g', label=\"Validation\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Cross-entropy loss')\n",
        "plt.grid()\n",
        "leg = plt.legend(loc='best', ncol=2, mode=\"expand\", shadow=False, fancybox=False)\n",
        "leg.get_frame().set_alpha(0.99)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvJhSMAzDMLyYUjAMwzC8/D/jBrW9TiBYaQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g-VGQ2pMavm4",
        "outputId": "96e3ef2f-bff9-445a-aab2-fc27b41ca97a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "x = list(range(len(train_accuracies)))\n",
        "\n",
        "ax = plt.subplot(111)\n",
        "plt.plot(x, train_accuracies, 'r', label=\"Train\")\n",
        "plt.plot(x, val_accuracies, 'g', label=\"Validation\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "leg = plt.legend(loc='best', ncol=2, mode=\"expand\", shadow=False, fancybox=False)\n",
        "leg.get_frame().set_alpha(0.99)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8oktkpkuqjet"
      },
      "source": [
        "**Questions**:\n",
        "\n",
        "*   What do you observe in the previous graphs?\n",
        "*   At which epoch is it interesting to retrieve the model parameters for inference?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwhRt39yzug-",
        "colab_type": "text"
      },
      "source": [
        "* The model starts overfitting.\n",
        "* ~20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XK_eUsq3avm8"
      },
      "source": [
        "# How to evaluate a model on the test set?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4UREO5elavm8"
      },
      "source": [
        "We can finally evaluate our model on our test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pPWvDM-qavm8",
        "outputId": "d31a6e1e-84b7-4fa7-892b-6c12934cf9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_loss, test_acc = evaluate(neural_net, test_loader, device)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eval:  Avg_Loss: 0.50465   Acc: 169/209 (80.861%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EvP_-KUwqjez"
      },
      "source": [
        "**Questions**:\n",
        "\n",
        "a) Compare validation and test metrics. <br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov4CKUFh5tun",
        "colab_type": "text"
      },
      "source": [
        "Validation and test metrics are comparable (test actually outperforms validation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfykotIrJV0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}